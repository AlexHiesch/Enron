{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Enron.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzfYnw0dJmlc"
      },
      "source": [
        "# Intro: Der Fall Enron\n",
        "\n",
        "Nach dem wir uns Vorgehensweisen und unterschiedliche Technologien angeschaut haben, ist es nun an der Zeit uns einer ersten Projektidee zu widmen und das Datenbeschaffen zu üben.\n",
        "\n",
        "Dies machen wir im Rahmen von Enron, einem der größten Wirtschaftsskandale in der Geschichte.\n",
        "\n",
        "## Zum Hintergrund von Enron:\n",
        "Enron galt einst einmal als eines der wertvollsten Vorzeigeunternehmen bis es 2001 zur Auflösung auf Grund von dreisten Bilanzfälschungen kam. 22000 Mitarbeiter verloren von einem Tag auf den anderen ihren Job. Millionen Privatinvestoren ihre Investitionen und Aktienanteile.\n",
        "\n",
        "Der Fall erregte großes Aufsehen und führte zu drastischen Verschärfungen in den gesetzlichen Vorschriften zur Unternehmensberichterstattung. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77PzyFcnGy9h"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA9qcrxwavZM"
      },
      "source": [
        "# Aufgabe 1: Daten herunterladen\n",
        "\n",
        "Um die Idee nun aufzugreifen, benötigen wir Daten. \n",
        "Im Allgemeinen fallen uns drei Beschaffungsmöglichkeiten ein, um an Daten heranzukommen:\n",
        "\n",
        "A) Die Daten liegen im Netz zum direkten Download bereit (wenn wir Glück haben sogar im kuratierten Zustand). Dabei können die Daten beispielhaft in folgenden Formaten sein:\n",
        "\n",
        "- strukturiert: csv, xls, xlsx, parquet, orc \n",
        "- semistrukturiert: html, json, yaml, xml, rdf, sql \n",
        "- unstrukturiert: ppt, pptx, pdf, docx, zip, pst, png, img, mp3, avi, mp4 \n",
        "\n",
        "B) Die Daten werden über eine Schnittstelle (API) angeboten und können darüber abgegriffen werden\n",
        "\n",
        "C) Die Daten, die uns interessieren sind nicht einfach herunterladbar und wir müssen sie von Webseiten aus der HTML-Repräsentation abgreifen (web scraping).\n",
        "\n",
        "\n",
        "## Enron Dataset\n",
        "Im Fall von Enron haben wir Glück.\n",
        "Beim Aufrollen des Falles wurden von 158, weitesgehend aus dem Management, 0,6 Mio E-Mail Nachrichten gesammelt und im Nachgang über die Federal Energy Regulatory Commision der Öffentlichkeit zur Verfügung gestellt.\n",
        "\n",
        "Die Daten sind u.a. hier zu finden:\n",
        "https://www.cs.cmu.edu/~./enron/\n",
        "\n",
        "Seitdem wird dieser Enron-Datensatz auch gerne für eDiscovery/EDA/ML/NLP/Spam oder im Rahmen von Research verwendet.\n",
        "\n",
        "Der Datensatz ist so populär, dass es dafür sogar einen eigenen Wikipedia-Artikel dazu gibt:\n",
        "https://en.wikipedia.org/wiki/Enron_Corpus\n",
        "\n",
        "Wir möchten als erstes  die Datei herunterladen. Dafür gibt es in Python zahlreiche Module. Ein sehr populäres ist `requests`. \n",
        "`requests` macht es uns einfach Webseiten mit Python anzusteuern. Für dieses Modul wird in der Regel kein Spitzname verwendet. Importiere `requests`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNNwvA6eJdwY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7mVHieJbT0y"
      },
      "source": [
        "Daten werden im Internet meist mit Hilfe von HTTPS (Hypertext Transfer Protocol Secure) übertragen. Das ist eine verschlüsselte Variante von HTTP ([Hyptertext Transfer Protocol](https://de.wikipedia.org/wiki/Hypertext_Transfer_Protocol)). Dabei wird durch den Browser eine Anfrage an den Server geschickt und dieser schickt eine Antwort zurück.\n",
        "\n",
        "Durch `requests` können wir die Anfrage auch ohne Browser an den Webserver schicken. Dazu stellt das Modul die Funktion `requests.get()` bereit. Speichere die Adresse einer Zip-Datei als *string* in der Variablen `website_url`. Überreiche diese Variable an `requests.get()` und speichere das Ergebnis als `response`. Drucke es anschließend aus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtAUvaK6CImK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4vFAEHynVuN"
      },
      "source": [
        "Hinweis: Falls der Download über https://www.cs.cmu.edu zu lange dauert, gibt es hier eine Alternative: https://github.com/AlexHiesch/Enron/blob/master/enron.tar.gz?raw=true\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrVeVFx6dcKV"
      },
      "source": [
        "Du solltest etwas wie `<Response [200]>` erhalten haben. Was bedeutet das jetzt? Auf unsere Anfrage, uns alle Informationen zum Abbilden der Wikipediaseite zu senden, haben wir eine Antwort erhalten. Diese hat den Statuscode 200. Für Statuscodes gibt es einige Standardcodes. Sie lassen sich in 5 Kategorien einteilen, die durch die erste Zahl angegeben werden:\n",
        "\n",
        "| Zahlenraum | Bedeutung |\n",
        "| ---------- | --------- |\n",
        "| 1XX | Die Anfrage wurde erhalten (Information) |\n",
        "| 2XX | Die Anfrage wurde erhalten und akzeptiert (Erfolgreiche Anfrage) |\n",
        "| 3XX | Weitere Aktionen werden durchgeführt, um die Anfrage zu erfüllen (Umleitung) |\n",
        "| 4XX | Die Anfrage kann nicht umgesetzt werden, was vermutlich am Client liegt (Client-Fehler) |\n",
        "| 5XX | Die Anfrage kann nicht umgesetzt werden, was vermutlich am Server liegt (Server-Fehler) |\n",
        "\n",
        "Die nächsten beiden Zahlen spezifizieren die Antwort. Eine umfangreiche Liste der Codes kannst du [hier](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) finden.\n",
        "\n",
        "Wir haben eine 200 erhalten. Dieser Code steht dafür, dass alles rund gelaufen ist. Weitere wichtige Codes sind folgende:\n",
        "* 401 (der Zugriff auf die gesuchten Informationen wurde verweigert, evtl. wegen fehlender/fehlerhafter Authentifizierung)\n",
        "* 404 (die gesuchten Informationen wurden nicht gefunden)\n",
        "* 429 (zu viele Anfragen wurden in einem bestimmten Zeitfenster gestellt)\n",
        "\n",
        "Schauen wir uns unsere Antwort nun etwas genauer an. Welchen Datentyp hat sie?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqtJaqHGdbuQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVkbTaOCd0_9"
      },
      "source": [
        "`response` hat den Typen `Response`. Eine `Response` enthält einige Informationen, die in unterschiedlichen Attributen stecken. Der Statuscode steckt als `int` im Attribut `my_response.status_code`. Wenn wir automatisierte Anfragen stellen, macht es Sinn den Status zu überprüfen, bevor wir weiter damit arbeiten. Dazu bietet uns `requests.codes` alle Codes für Menschen lesbar an. So können auch Personen den Code verstehen, die keine oder nur wenig Erfahrung mit Webseiten haben.\n",
        "\n",
        "Wir werden nur den Code `requests.codes.ok` brauchen. Er sagt uns, dass alles ok ist (Code 200). Vergleiche diesen mit `response.status_code`, sind sie gleich?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stSyFL2ed2xT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1OnBxFhd9mf"
      },
      "source": [
        "Der Statuscode sollte gleich  zu `requests.codes.ok` sein. Wenn wir auf Nummer sicher gehen möchten, können wir eine Fehlermeldung ausgeben, falls der Statuscode nicht passt. Dazu haben wir die Methode `my_response.raise_for_status()`. Das probieren wir mal aus. Nutze `requests`, um die Website-Daten von https://httpbin.org/status/404 zu erhalten. Speichere die Antwort als `response_404` ab und nutze die Methode `my_response.raise_for_status()`. Was passiert?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdWzkKy3d9M_"
      },
      "source": [
        "website_url = 'https://httpbin.org/status/404'\n",
        "response_404 = requests.get(website_url)\n",
        "response_404.raise_for_status()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb83834Re2o1"
      },
      "source": [
        "Wir erhalten die Fehlermeldung `HTTPError: 404 Client Error: NOT FOUND for url: https://httpbin.org/status/404`. Den Statuscode 404 hast du vielleicht schon einmal in deinem Browser gesehen, wenn eine Website nicht erreichbar ist. Was passiert, wenn du versuchst eine Fehlermeldung für die Wikipediaseite, also mit `response`, zu erzeugen?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv7lxZR_e39g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1l6oWDwfP5k"
      },
      "source": [
        "Bei mir ist nichts passiert. Das ist ein gutes Zeichen! Wenn `my_response.raise_for_status()` keine Fehlermeldung erzeugt, dann wurde die Anfrage vom Server verarbeitet und wir haben eine Antwort erhalten, mit der wir weiterarbeiten können.\n",
        "\n",
        "Neben dem Statuscode können wir weitere Informationen aus unserer Antwort ziehen. Drucke das Attribut `my_response.headers`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpbTaKZ4fR1_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DshG81Doff9m"
      },
      "source": [
        "Der Output ist auf den ersten Blick etwas unübersichtlich. Aber an den geschweiften Klammern erkennen wir, dass es sich vermutlich um ein *dictionary* handelt. Das stimmt so aber noch nicht ganz. Da die Headerdaten für die Kommunikation mittels HTTP als *case-insensitive* definiert wurden, gilt das auch für `my_response.headers`. Es ist also egal, ob die Buchstaben der *keys* groß oder klein sind.\n",
        "`my_response.headers` enthält Metadaten zu der Antwort, die wir erhalten haben. Welche Werte stecken hinter den *keys* `'content-type'` und `'Last-Modified'`? Gib sie aus.\n",
        "\n",
        "Tipp: Wie eben erwähnt sind die *keys* hier, im Gegensatz zum normalen `dict`, nicht anfällig für Groß- und Kleinschreibung. Es ist also egal, ob du `'content-type'` oder `'CONTENT-type'` schreibst."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo5dOnRSfhFf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcA0aptpcWCn"
      },
      "source": [
        "Vergleiche die `'content-length` mit dem tatsächlich erhalten Inhalt in `response.content` in MB. Sind die Angaben identisch? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODCr222ODGwt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aEJ2XrjdCJn"
      },
      "source": [
        "Der Download läuft über Colab statt über unsere Internetleitung zuhause.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XfZO0auhSXV"
      },
      "source": [
        "Lasst uns das Ganze erst mal aus unserem Speicher über ein file handler `with open(Dateizumrausschreiben) as file:` und der `write`-Methode lokal auf die Festplatte unter `enron.tar.gz` ablegen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f78K269VhcN6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52EQSAGBGwbz"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4lMETCoCnCn"
      },
      "source": [
        "# Aufgabe 2: Daten entpacken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xksVuV6Minp1"
      },
      "source": [
        "Ob dies auch geklappt hat, sehen wir über `os.listdir()`. Vergesst den notwendigen import vorher dafür nicht."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCkaLz4KhjpU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbCpC3fYi2kT"
      },
      "source": [
        "Jetzt heißt es, die gepackte Datei zu entpacken. Auch hier wieder gibt es viele Module in Python. Wir importieren dafür `tarfile` und benutzen die Methode `open` in r:gz-Mode und `extractall`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGYob3VEiEPs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jn5v0s_tjlSM"
      },
      "source": [
        "Wir haben jetzt 2,6GB an Daten in entpackter Form. Ein erneutes `os.listdir(\"neues Verzeichnis\")` verrät uns die Dateistruktur. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIAM7S2djuQe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21qV9u9Mp2_U"
      },
      "source": [
        "Wie wir sehen, ist nach maildir alles nach den Mitarbeiterkürzeln angeordnet. Vielleicht wäre es interessantest direkt an der Spitze anzufangen und Nachforschungen anzustellen, welche E-Mail Inhalte für `lay-k` vorliegen. Dies war der CEO Kenneth Lee Lay, der zusammen mit Jeffrey Skilling ins Gefängnis musste.\n",
        "\n",
        "Wir benutzen hier os.walk, was uns drei Sachen bei einem gegeben Verzeichnis erforscht und zurückgibt: \n",
        "- Verzeichnis\n",
        "- Unterverzeichnis\n",
        "- Dateinamen\n",
        "\n",
        "Probier dies mal aus und druck die Werte aus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nMyZet5kTHm"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGQ3JlJ_Gt4n"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFrDuyjSsWpV"
      },
      "source": [
        "# Aufgabe 3: Daten einlesen\n",
        "\n",
        "\n",
        "Wir sehen die ganzen Daten so, wie sie damals im eingesetzten E-Mail Programm vorlagen. Die Dateien selbst MIME-Format:\n",
        "https://en.wikipedia.org/wiki/MIME\n",
        "\n",
        "Dies ist ein Standard, der sich für E-Mails entwickelt hat und inwischen auch darüber hinaus im Web als auch in Linux Desktops Einsatz findet. MIME ermöglicht \n",
        "\n",
        "Wir können die Dateien zwar nativ einlesen mit open, aber um hier nur den Content zu parsen müssen wir auch hier wieder ein Python-Model namens `email.parse` einsetzen. Dies können wir über `from email.parser import Parser` importieren.\n",
        "\n",
        "Probier das mit einer ersten E-Mail aus:\n",
        "\n",
        "Erstelle einen File-Handler wie für die tar.gz s.o. und lies die Datei `maildir/lay-k/all_documents/1` als Variable `data`\n",
        "ein, um es dann an `Parser().parsestr(data)` zu übergeben. Speichere die Rückgabe unter der Variable `email`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djOK_24vuoch"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7zEAfYFunpn"
      },
      "source": [
        "Die Metadaten sind jetzt einzeln abrufbar über das email-Objekt. \n",
        "\n",
        "Gib folgende Inhalte aus:\n",
        "\n",
        "`mail['to']`\n",
        "\n",
        "`email['from']`\n",
        "\n",
        "`mail['subject']`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDllFwZqvNGS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v05BD58UxZt_"
      },
      "source": [
        "Der eigentliche Inhalt der E-Mail ist im Body. Diesen erhält man über `email.get_payloud()`\n",
        "\n",
        "Gib ihn bitte aus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylhx3CGexi2Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YadQ0CF5hD_"
      },
      "source": [
        "--- "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apJgP-iU5h9r"
      },
      "source": [
        "# Aufgabe 4: Metadaten auswerten"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDAr67wbxszT"
      },
      "source": [
        "Prima!\n",
        "\n",
        "Jetzt können wir E-Mails lesen und verbinden das mit oder `os.walk`-Methode von weiter oben, um als erster Mal zu analyisieren mit wem der CEO am häufigsten Kontakt hatte.\n",
        "\n",
        "Anbei hast du eine vordefinierte Funktion.\n",
        "\n",
        "Um diese zu nutzen erstelle bitte drei leere Listen:\n",
        "\n",
        "`to_email_list = []`\n",
        "\n",
        "`from_email_list = []`\n",
        "\n",
        "`email_body = []`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC9Qlw69zCCs"
      },
      "source": [
        "# hier Listen initialisieren\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EHOGUuDyEMV"
      },
      "source": [
        "def clean_string(s: str) -> str:\n",
        "  '''Funktion um Strings von Leerzeichen, Tabs und Umbrüchen zu bereinigen'''\n",
        "  s = s.replace(\"\\n\", \"\")\n",
        "  s = s.replace(\"\\n\\\\\", \"\")\n",
        "  s = s.replace(\"\\t\", \"\")\n",
        "  s = s.replace(\" \", \"\")\n",
        "  return s\n",
        "\n",
        "def email_analyse(inputfile, to_email_list, from_email_list, email_body):\n",
        "  '''Funktion um jeweilige E-Mails zu parsen und daraus eine Absender-/Empfänger- \n",
        "  als auch eine E-Mail-Body-Liste zu erstellen''' \n",
        "  with open(inputfile, \"r\",errors=\"ignore\") as f:  \n",
        "      data = f.read()\n",
        " \n",
        "  # email to enthält unter Umständen viele Empfänger und muss separat \n",
        "  # behandelt werden \n",
        "  email = Parser().parsestr(data)\n",
        "\n",
        "  if email['to']:\n",
        "    email_to = clean_string(email['to'])\n",
        "    email_to = email['to'].split(\",\")\n",
        "\n",
        "    for email_to_1 in email_to:\n",
        "      email_to_1 = clean_string(email_to_1)\n",
        "      to_email_list.append(email_to_1)  \n",
        "\n",
        "  from_email_list.append(email['from'])\n",
        "\n",
        "  email_body.append(email.get_payload())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO6kDQeJzFYx"
      },
      "source": [
        "# wir iterieren nun über alle E-Mail vom CEO und benutzen dabei os.path.join um auch den absoluten Dateipfad zu erhalten\n",
        "# jede einzelne E-Mail jagen wir über die email_analyse-Funktion, dafür bist du wieder hier an der Reihe\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUQzuStN6BSC"
      },
      "source": [
        "# Mit wie vielen Personen stand der CEO in Kontakt?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU9x_h0Bzq7h"
      },
      "source": [
        "# Wir werten die Top 10 Absender und Empfänger aus\n",
        "import collections\n",
        "print(\"\\nTo email adresses: \\n\")\n",
        "print(collections.Counter(to_email_list).most_common(10))\n",
        "\n",
        " \n",
        "print(\"\\nFrom email adresses: \\n\")\n",
        "print(collections.Counter(from_email_list).most_common(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVJaitBRZnF6"
      },
      "source": [
        "Versucht beide Listen nun zu visualisieren"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4KxcLbr3BcA"
      },
      "source": [
        "# Top 10 E-Mail Empfänger"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOc60pXJgHHb"
      },
      "source": [
        "# Top 10 E-Mail Sender"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRzUuN8CfjbF"
      },
      "source": [
        "hmm die meisten E-Mails stammen von Rosalee Flemming. Könnt ihr ahnen weshalb das so war?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsG06skvG3Tw"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g3_-yTSGTQY"
      },
      "source": [
        "# Aufgabe 5: Daten aufbereiten über POS-Tagging\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1uDEAmyHanI"
      },
      "source": [
        "*Natural Language Processing*, kurz NLP, ist ein Teilgebiet der Informatik und künstlichen Intelligenz, das sich mit der Interaktion zwischen Computern und menschlichen (natürlichen) Sprachen befasst.\n",
        "\n",
        "Im Deutschen nennen wir dies auch Computerlinguistik.\n",
        "https://de.wikipedia.org/wiki/Computerlinguistik\n",
        "\n",
        "Viele der uns bekannten Technologien basieren auf Anwendungen von NLP. Zum Beispiel haben viele Smartphones und Online-Suchmaschinen Textvorhersagefunktionen, die vorhersagen, was du eingeben möchtest. Das machen sie basierend auf den Buchstaben oder Wörtern, die du bereits getippt hast. Online-Übersetzungstools basieren auf NLP-Techniken, um Wörter zwischen Sprachen zu übersetzen. Die automatische Rechtschreibprüfung ist ebenfalls ein Produkt von NLP.\n",
        "\n",
        "In unserem Beispiel gibt es bestimmt viele interessante Möglichkeiten, die wir hier umsetzen könnten. Spontan fällt mir hier ein: \n",
        "\n",
        "- Welche Wörter treten am häufigsten auf?\n",
        "- Zu welchen Uhrzeiten werden die meisten E-Mails empfangen/versendet?\n",
        "- Wer schreibt/empfängt die kürzesten/längsten E-Mails?\n",
        "- Zu welchen Themen (Topic Modeling)?\n",
        "- Können wir die E-Mails auch klassifizieren hinsichtlich SPAM/Kein-SPAM? (Text-Tagging)\n",
        "- Welches Sentiment haben die meisten E-Mails (Natural Language Understanding)?\n",
        "- Wie hat sich dies über die Zeit hin entwickelt?\n",
        "- Können wir anhand von bestimmten Wörtern oder Topics Beweise ausfinden machen für die damaligen Straftaten?\n",
        "- Lassen sich Graphen erstellen um Beziehungsmuster in der Organisation zu identifizieren?\n",
        "- Können E-Mail Text antrainiert werden, so dass man komplett neue Texte kreieren kann? (Natural Language Generation)\n",
        "\n",
        "Und vieles mehr....ihr habt bestimmt auch einige Ideen, die euch in den Sinn kommen.\n",
        "\n",
        "Da wir uns mit NLP jedoch nur am Rande befassen, beschränken wir es hier an der Stelle bei der Textaufbearbeitung und einer einfachen Analyse. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz05dMAES-vW"
      },
      "source": [
        "Gib hierzu die letzten 20 E-Mails vom CEO aus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM-bkgzzHzNi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDEaglHhOdv_"
      },
      "source": [
        "Starker Tobak\n",
        "\n",
        "Man liest regelrecht die Untergangsstimmung und Verzweiflung aus den Texten heraus:\n",
        "\n",
        "- *I realize that times are tough, but this is really twisting the knife that is already deeply embedded in the majority of employees.  Have you ever gone down to the Body Shop in the mornings or at noon to see how many employees use the facility for health and stress relief purposes?  We've done a lot of things during the past few weeks to kill morale, but this is a new low.  Are you going to do anything about it?  I think that a lot of people would like to know.*\n",
        "\n",
        "\n",
        "- *The NYSE's delisting of our stock will not have an impact on Enron's business, and we will continue to trade in the OTC market.*\n",
        "\n",
        "- *The road to Enron was not an easy one, and  my  decision to seek employment here was difficult in light of the way Enron is portrayed in the back offices of the energy industry.  Enron is said to be a sweatshop, a backstabbing work environment and that the executive levels of management at Enron fuel, foster and support this type of mentality.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSYiJyG7UBrs"
      },
      "source": [
        "In Python gibt es eine Reihe von NLP-Modulen. In diesem Kapitel verwenden wir `spacy` ([siehe offizielle Dokumentation](https://spacy.io/)) und `nltk` ([siehe offizielle Dokumentation](https://www.nltk.org/)). Importiere diese beiden Module in der untenstehenden Zelle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nFbZBbKUSg3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXheOZNxUmK1"
      },
      "source": [
        "`spacy` und `nltk` bieten ähnliche Werkzeuge für NLP, haben aber unterschiedliche Ansätze. \n",
        "\n",
        "Funktionen aus `nltk` nehmen `str`-Werte entgegen und geben `str`-Werte als Ausgabe zurück. `spacy` verwendet einen objektorientierten Ansatz und gibt meist Dokumentenobjekt mit eigenen Attributen und Methoden zurück. Viele Anwender empfinden `spacy` als zeit- und speichereffizienter als `nltk` und damit als besser geeignet für die Produktion. \n",
        "\n",
        "Aus diesem Grund werden wir `spacy` verwenden, um die meisten NLP-Aufgaben zur Bereinigung von Textdaten durchzuführen, mit Unterstützung bestimmter Funktionen und Methoden von `nltk`, `re` und `string`.\n",
        "\n",
        "Um dies zu tun, brauchen wir ein statistisches Modell von `spacy`. Die Wahl des Modells sollte von der Sprache des zu analysierenden Korpus abhängen. Die vollständige Liste findest du [hier](https://spacy.io/usage/models). \n",
        "\n",
        "Importiere `sentences` von `spacy.lang.en.examples` und lade das statistische Modell für Englisch `en_core_web_sm` als `str` mit der Funktion `spacy.load()`. Speichere das Ergebnis in der Variable `nlp`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Edu2mx9OUVZI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTAVrPWGUxx0"
      },
      "source": [
        "`nlp` hat den Datentyp `Language`, welcher alle Komponenten enthält, die zur Verarbeitung von englischsprachigem Text benötigt werden. Gib den Typ aus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDXuWR48U1gU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cxv7bt9uVDA3"
      },
      "source": [
        "Um Textdaten richtig zu analysieren, sollten Modelle des maschinellen Lernens in der Lage sein, Strukturen im Text zu erkennen, wie z.B. einzelne Wörter und deren Sprachanteile. Deshalb müssen wir zunächst eine Tokenisierung durchführen, bei der der Korpus in sinnvolle linguistische Einheiten, wie Wörter oder Sätze, unterteilt wird. Diese werden dann *token* genannt.\n",
        "\n",
        "In unserem Beispiel, macht es Sinn, die Daten in einzelne Wörter zu trennen, da wir bereits viele Textnachrichten haben.\n",
        "\n",
        "Um dies zu tun, müssen wir ein `Doc`-Objekt erstellen, das den Text im Korpus enthält. Dazu nutzen wir das `Language`-Objekt (in unserem Beispiel ist das die Variable `nlp`) und überreichen ihm den Text, den wir analysieren möchten. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlTH8vq5XTt4"
      },
      "source": [
        "Wir schauen uns mal die 2. Mail an und wandeln sie in ein `Doc`-Objekt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z830l_zRVRyC"
      },
      "source": [
        "print(email_body[2])\n",
        "doc = nlp(email_body[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DkQnaNWaRhL"
      },
      "source": [
        "Ein `Doc`-Objekt ist eine Folge von `token`-Objekten, die die einzelnen linguistischen Einheiten sind, die wir für unsere Analyse benötigen. Durch sie können wir mit einer `for`-Schleife durchiterieren. der Text eines `token` steckt im Attribut `my_token.text`.\n",
        "\n",
        "In der folgenden Zelle erzeuge bitte mit Hilfe einer *list comprehension* eine Liste mit den Textinhalten der einzelnen `token`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B10pzEjUaSvX"
      },
      "source": [
        "doc_tokens = [token.text for token in doc]\n",
        "\n",
        "print(doc_tokens)\n",
        "print(len(doc_tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTfGdrS4bSNo"
      },
      "source": [
        "`doc` ist nun in Wörter und Satzzeichen unterteilt. `spacy` erkennt einzelne `token` vor allem durch die Trennung durch Leerzeichen. Es gibt jedoch einige Ausnahmen. Zum Beispiel würden Kontraktionen von zwei Wörtern im Englischen wie *\"I'll\"* als \"I\" und *\"will\"* tokenisiert werden. [Hier](https://spacy.io/usage/spacy-101#annotations-token) kannst du mehr darüber erfahren, wie Tokenisierung mit `spacy` funktioniert."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKcBbdTabvHa"
      },
      "source": [
        "Führe diesen Vorgang bitte für die 5093. Mail aus und benutze hierfür andere Variablennamen. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1Zzn3xwbS8f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUnIFW3BcRBZ"
      },
      "source": [
        "`spacy` teilt eine Nachricht nicht nur auf, sondern führt auch *parts of speech tagging* (POS-Tagging) durch. Für jeden `token` wird also angegeben, um welche Art Wort es sich handelt, wie z.B. Substantive (`'NOUN'`) oder Satzzeichen (`'PUNCT'`).\n",
        "\n",
        "Darauf können wir mit dem Attribut `my_token.pos_` zugreifen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCZJ9t4tcSQZ"
      },
      "source": [
        "token_pos = [[token.text, token.pos_] for token in doc]\n",
        "print(token_pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sn9VTh-ctSJ"
      },
      "source": [
        "Probier das auch mal mit der 5093. Mail."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfGD7HsMcx2K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEa0d1f2deQ9"
      },
      "source": [
        "Die vollständige Liste der Tags und deren Bedeutung findest du [hier](https://universaldependencies.org/u/pos/index.html). Diese Tags basieren auf einem Standard für POS-Tags, die im Projekt [Universal Dependencies](https://universaldependencies.org/) definiert werden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji5Gki2qfPGL"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQYBQzu4dX66"
      },
      "source": [
        "# Aufgabe 6: Textdaten bereinigen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui_evPTrfEbH"
      },
      "source": [
        "Textdaten können in verschiedenen Formen vorliegen: in Büchern, Zeitschriften, Webseiten, Online-Nachrichten usw. Auch die Länge des Textes variiert: Textnachrichten sind in der Regel viel kürzer als Romane. Damit unsere Modelle mit ihnen Umgehen können, müssen all diese Textformen zuerst aufbereitet und in ein Format, das aus Zahlen besteht, überführt werden.\n",
        "\n",
        "Einige Reinigungs- und Aufbereitungstechniken sind:\n",
        "1. Lemmatisierung \n",
        "2. Stoppwort-Entfernung\n",
        "3. Entfernung von Satzzeichen\n",
        "\n",
        "Lemmatisierung bedeutet, dass Wörter auf ihre Grundform reduziert werden, auch bekannt als *lemma*. Aus grammatikalischen Gründen können verschiedene Formen desselben Wortes in einem Text verwendet werden, z.B. *make*, *makes*, *making* oder *maker*. Python würde diese Variationen des Wortes *make* als getrennte Wörter betrachten, obwohl ihre Bedeutung die gleiche ist. Durch die Lemmatisierung bringen wir alle Variationen von *make* auf diese Grundform.\n",
        "\n",
        "Mit dem Attribut `my_token.lemma_` können wir die Grundform des entsprechenden `token` erhalten. Dabei wird POS-Tagging verwendet, um das passende Lemma zu bestimmen. Führe folgende Zelle aus, um die Grundformen der Wörter in `doc` als Liste zu erhalten."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_YL8rG0itrZ"
      },
      "source": [
        "type(nlp(email_body[2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPo2XlcIfP--"
      },
      "source": [
        "# Original\n",
        "print(\"ORIGINAL TEXT:\\n\", doc)\n",
        "\n",
        "# die Lemmas von den doc Elementen erhalten und ausgeben\n",
        "lemma_token = [token.lemma_ for token in doc]\n",
        "print(\"AFTER LEMMATIZATION:\\n\",lemma_token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tRFTqvoj3br"
      },
      "source": [
        "Pronomen werden in `spacy` als `'-PRON-'` lemmatisiert. Möchten wir diese nicht in der Liste haben, können wir der *list comprehension* eine `if`-Anweisung hinzufügen. Hier überschreiben wir `doc` mit der Liste der *strings*, die wir aus dem Lemmata erhalten. Diese Liste bereinigen wir dann Schritt für Schritt ohne die Daten als `token`-Dateityp zu benötigen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6mzqQy3j5Fq"
      },
      "source": [
        "doc_lemma = [token.lemma_ for token in doc if token.lemma_ != \"-PRON-\"]\n",
        "print('\\n')\n",
        "print(doc_lemma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgvbwiqOkiJt"
      },
      "source": [
        "Mache dies nun auch für die 5093. Mail"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeXeTr7qkoqZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozceY7R9k8Te"
      },
      "source": [
        "*Stoppwort-Entfernung* beinhaltet gängige Wörter aus einem Text zu entfernen. Stoppwörter sind in der Regel Artikel (\"the\" und \"a\"), Pronomen wie \"I\" und \"you\" (die bereits im vorherigen Schritt entfernt wurden) oder gängige Verben (\"be\", \"can\"). Diese Wörter kommen in den meisten Texten häufig vor. Das Entfernen dieser Wörter würde die Datenmenge reduzieren, die analysiert werden muss und gleichzeitig ermöglichen, dass maschinelle Lernalgorithmen mehr Gewicht auf Token legen, die einem Text seine echte Bedeutung geben.\n",
        "\n",
        "Verwenden wir für diese Übung die Stoppwörter, die vom Modul `nltk` bereitgestellt werden. Führe die folgende Zelle aus, um sie zu importieren, zu speichern und zu drucken. Dabei wandeln wir die Stoppwörter in ein `set` um. Das ist ein Datentyp in Python, der dafür optimiert wurde, Elemente darin zu suchen. Wenn wir also später  überprüfen, ob ein bestimmtes Wort ein Stoppwort ist, dann ist unser Code mit einem `set` viel schneller als mit einer `list`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3NAVGWyk-5E"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# import stopwords from nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# convert stopwords to set\n",
        "stopWords = set(stopwords.words('english'))\n",
        "\n",
        "# print stopwords\n",
        "print(stopWords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EEy2BWplMm6"
      },
      "source": [
        "Anstatt eine `token`-Methode aus `spacy` anzuwenden, verwenden wir einfach eine *list comprehension* mit einer `if`-Anweisung, um nur Wörter zu erhalten, die nicht im Satz der Stoppwörter enthalten sind."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA4n1hPylPEv"
      },
      "source": [
        "# Nachricht mit Stopwords\n",
        "print(\"LEMMATIZED TEXT:\\n\", doc_lemma)\n",
        "\n",
        "# Nachricht ohne\n",
        "doc_lemma_stop = [token for token in doc_lemma if token not in stopWords ]\n",
        "print(\"NO STOP WORDS:\\n\", doc_lemma_stop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyu3mBnslumw"
      },
      "source": [
        "Genau wie die Stoppwort-Entfernung beinhaltet die *Satzzeichen-Entfernung* das Entfernen von Satzzeichen und Symbolen, die nicht zur Bedeutung des Textes beitragen. Wir können `punctuation` aus dem Modul`string` verwenden. Dabei handelt es sich um einen *string*, der aus Satzzeichen und Symbolen besteht. Diese können wir wie die Stoppwörter aus unserem Text entfernen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-OfJlWplwO1"
      },
      "source": [
        "import string\n",
        "punctuations = string.punctuation\n",
        "print(punctuations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpWz6vRGl96N"
      },
      "source": [
        "Um die Satzzeichen zu entfernen, gehen wir so vor, wie bei den Stoppwörtern: Wir nutzen eine *list comprehension* mit `if`-Abfrage. Dabei können wir durch einen *string* iterieren, als ob es eine Liste wäre. Python greift dann auf die einzelnen Buchstaben zu und erlaub nur vergleiche mit *strings*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPFL8U91mAxI"
      },
      "source": [
        "# print message\n",
        "print(\"TEXT WITH NO STOP WORDS:\\n\", doc_lemma_stop)\n",
        "\n",
        "# remove punctuations\n",
        "doc_lemma_stop_punct = [token for token in doc_lemma_stop if token not in punctuations and token != \"\\n\"]\n",
        "print(\"NO PUNCTUATIONS:\\n\", doc_lemma_stop_punct)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o38-P_NusO0W"
      },
      "source": [
        "Jetzt bist du wieder an der Reihe und bereinige jetzt auch die 5093. E-Mail:\n",
        "\n",
        "- Stopwords entfernen\n",
        "- Satzzeichen entfernen\n",
        "- Ausgabe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9EYXcsFrqi8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiNyNQiquB-D"
      },
      "source": [
        "--- "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8N6HobGt57p"
      },
      "source": [
        "# Aufgabe 7: Eine benutzerdefinierte Funktion zur Aufbereitung und Bereinigung"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooMTzsdHuMx4"
      },
      "source": [
        "Im vorherigen Abschnitt wurden für die Reinigung und Vorbehandlung mindestens 4 Schritte durchgeführt. Diesen Vorgang wollen wir auf jede Nachricht anwenden, deshalb ist es am besten, eine eigene Funktion für diese Aufgaben zu erstellen.\n",
        "\n",
        "Folge der untenstehenden Vorlage von `text_cleaner()`, die einen `str`-Wert `sentence` entgegennimmt. Vorerst sollte die Funktion eine `list` aus *strings* zurückgeben. Nutze dieselben Schritte, wie in der letzten Codezelle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISmNjADbuLsN"
      },
      "source": [
        "import re\n",
        "# Definiere die Funktion `text_cleaner()` mit dem Parameter `sentence` \n",
        "def funktionsname(parameter):\n",
        "    # Erstelle das Doc-Objekt `sentence` unter Verwendung von `nlp()`\n",
        "     \n",
        "    # Lemmatisierung\n",
        "     \n",
        "    # Stoppwort Entfernung\n",
        "     \n",
        "    # Satzzeichen Entfernung\n",
        "     \n",
        "    # Ausgabe    \n",
        "    return doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv1ev9pOugjI"
      },
      "source": [
        "Führe die untenstehende Zelle aus, um zu überprüfen, ob die Funktion ordnungsgemäß funktioniert. Das Ergebnis sollte das gleiche sein wie bei der vorherigen Übung."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD9TGuvPulYy"
      },
      "source": [
        "print(text_cleaner(email_body[5093]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu4XZaUN_qsI"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2pWZu6l_wnc"
      },
      "source": [
        "# Aufgabe 8: Text-Sentiment-Analyse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifjBkRK4JVSj"
      },
      "source": [
        "Die Sentimentanalyse ist ein Untergebiet des Text Mining und versucht automatisch die geäußerte Haltung als positiv oder negativ zu erkennen.\n",
        "\n",
        "In unserem Fall bedienen wir uns einem vorab trainiertem Modell namens Valence Aware Dictionary and sEntiment Reasoner ([VADER](https://github.com/cjhutto/vaderSentiment)) aus dem NLTK-Modul. VADER ist lexika- und regelbasiert und sepziell auf Social Media-Inhalte ausgelegt. Dies passt zwar in unserem Kontext nicht zu 100%, da unsere Texte womöglich länger und förmlicher sind aber dies rechtfertigt wieder der geringe Aufwand, den man hier unternehmen muss.\n",
        "\n",
        "Alternativ gibt es von diversen Hyperscalern auch kostenpflichtige APIs, die man hierfür einsetzen könnte.\n",
        "\n",
        "\n",
        "- Zuerst müsst ihr über `nltk.download()` die `vader_lexicon` herunterladen.\n",
        "- Anschließend importiert ihr den `SentimentIntensityAnalzyer` aus dem `nltk.sentiment`-Modul.\n",
        "- `SentimentIntensityAnalzyer` muss instantiiert werden, was ihr am besten als Variable `sia` zurückgebt\n",
        "- Probiert mal erste Einschätzungen über `sia.polarity_scores(\"< hier Beispielsatz einfügen>\")`\n",
        "Ihr erhaltet vier Werte zurück.\n",
        "  - *compound* = normalisierte, gewichtite zusammengesetzte Punktzahl zwischen -1 und +1\n",
        "  - *neg* = negativer Wert\n",
        "  - *neu* = neutraler Wert\n",
        "  - *pos* = positiver Wert\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWyXzpIkTXuo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrJ3Lm2cYEnD"
      },
      "source": [
        "Probiert das Ganze jetzt einmal auf die 5093. E-Mail vom CEO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkOPBGS7_2XP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZB4TbsVYeje"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M1aDxuYzsWc"
      },
      "source": [
        "# Aufgabe 9: Kompletten Datensatz auswerten"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eea3pRdLYh4d"
      },
      "source": [
        "Nachdem wir uns einzelne E-Mails angeschaut haben, ist es an der Zeit uns dem kompletten Datensatz zu widmen.\n",
        "\n",
        "Hierfür iterieren wir über die `email_body`-Liste vom CEO und übergeben die jeweiligen Inhalte an die zuvor definierte Bereinigungsfunktion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTsSmugi0DNt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr_IgR7g3ocJ"
      },
      "source": [
        "# Wir entfernen hier noch Leerzeichen, Zeilenumbrüche über einen Regularen Ausdruck\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_pS51z90bIM"
      },
      "source": [
        "# Gib die 20 häufigsten Wörter in den E-Mails vom CEO\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItnEr0U16MM3"
      },
      "source": [
        "# Erstelle eine Wordcloud über https://pypi.org/project/wordcloud/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d78DLOqz7gmQ"
      },
      "source": [
        "# Abschlussplot: Plote in einer ansprechenden Darstellung das Gesamtergebnis\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC1iR3zuYjQT"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAhUHrVtH3aR"
      },
      "source": [
        "# Fazit\n",
        "\n",
        "Dies war nur der Anfang und soll einen ersten Eindruck geben, wie Daten programmatisch heruntergeladen und aufbereitet werden können.\n",
        "\n",
        "Bei einer Häufigkeitsanalyse würde sich auch eine Vektorisierung nach Bag of Words (BoW) und Term frequency-inverse document frequency(TF-IDF) anbieten. "
      ]
    }
  ]
}